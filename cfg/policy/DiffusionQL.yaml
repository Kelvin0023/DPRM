action_horizon: 4

# denoising steps
denoising_steps: 20

# Value and advantage estimation
normalize_value: True
value_bootstrap: True
gamma: 0.99
tau: 0.95


model:
  # HP to tune
  gamma_denoising: 0.99
#  clip_vloss_coef: 0.2
  clip_ploss_coef: 0.001
  clip_ploss_coef_base: 0.001
  clip_ploss_coef_rate: 3
  randn_clip_value: 3
  min_sampling_denoising_std: 0.1
  min_logprob_denoising_std: 0.1

#  use_ddim: True
#  ddim_steps: 5
#  learn_eta: False
#
#  eta:
#    base_eta: 1
#    mlp_dims: [ 256, 256 ]
#    min_eta: 0.1
#    max_eta: 1.0

  actor:
    time_emb_dim: 32
    mlp_dims: [1024, 1024, 1024]
    cond_mlp_dims: [512, 64]
    activation_type: ReLU
    out_activation_type: Identity
    use_layernorm: False
    residual_style: True

  critic:
    mlp_dims: [256, 256, 256]
    activation_type: Mish
    use_layernorm: False
    residual_style: True


trainer:
  # batch size
  batch_size: 512 #1024

  # mini epochs
  epochs: 30

  # grad clipping
  max_grad_norm: 1.0

  # Max Q Backup
  max_q_backup: False

  # value normalization
  normalize_value: False
  value_scale: 1.0

  # EMA for target actor
  step_start_ema: 1000
  ema_decay: 0.995
  update_ema_every: 5

  eta: 5.0 #10.0
  tau: 0.005
  discount: 0.99

  lr_decay: False
  lr_max_T: 1000

  actor_lr: 1e-4
  actor_weight_decay: 0
  actor_lr_scheduler:
    first_cycle_steps: 1000
    warmup_steps: 10
    min_lr: 1e-4
  critic_lr: 1e-3
  critic_weight_decay: 0
  critic_lr_scheduler:
    first_cycle_steps: 1000
    warmup_steps: 10
    min_lr: 1e-3